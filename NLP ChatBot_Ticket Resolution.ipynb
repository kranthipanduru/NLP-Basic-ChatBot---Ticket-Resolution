{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tickets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Resolution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Need Ethernet access</td>\n",
       "      <td>Please connect the LAN cable to the red port o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Create new account and share the username and ...</td>\n",
       "      <td>Go to URL : accountsetup.com, Enter personal d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FW: Myspace Login Issue-- Ania</td>\n",
       "      <td>Go to URL : password.reset.com, Enter you logi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Login credetials are not working.</td>\n",
       "      <td>Go to URL : password.reset.com, Enter you logi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New Account is needed for a new joinee.</td>\n",
       "      <td>Go to URL : accountsetup.com, Enter personal d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>System behaves strangely, blue screen appears ...</td>\n",
       "      <td>Press F8 button on system start, Choose safe m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>System showing a blue screen at startup</td>\n",
       "      <td>1. Hold the power button until system gets shu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>The account was blocked due to 3 wrong passwor...</td>\n",
       "      <td>Unlocked the account and new password is sent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>The computer will not turn on</td>\n",
       "      <td>Contact IT support.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>The desktop is not working</td>\n",
       "      <td>Please check the Power cables are properly plu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>226 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  \\\n",
       "0                                 Need Ethernet access   \n",
       "1    Create new account and share the username and ...   \n",
       "2                       FW: Myspace Login Issue-- Ania   \n",
       "3                    Login credetials are not working.   \n",
       "4              New Account is needed for a new joinee.   \n",
       "..                                                 ...   \n",
       "221  System behaves strangely, blue screen appears ...   \n",
       "222            System showing a blue screen at startup   \n",
       "223  The account was blocked due to 3 wrong passwor...   \n",
       "224                      The computer will not turn on   \n",
       "225                         The desktop is not working   \n",
       "\n",
       "                                            Resolution  \n",
       "0    Please connect the LAN cable to the red port o...  \n",
       "1    Go to URL : accountsetup.com, Enter personal d...  \n",
       "2    Go to URL : password.reset.com, Enter you logi...  \n",
       "3    Go to URL : password.reset.com, Enter you logi...  \n",
       "4    Go to URL : accountsetup.com, Enter personal d...  \n",
       "..                                                 ...  \n",
       "221  Press F8 button on system start, Choose safe m...  \n",
       "222  1. Hold the power button until system gets shu...  \n",
       "223  Unlocked the account and new password is sent ...  \n",
       "224                                Contact IT support.  \n",
       "225  Please check the Power cables are properly plu...  \n",
       "\n",
       "[226 rows x 2 columns]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets remove punctuations if any \n",
    "\n",
    "punct = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "sample_str = \"Hi! , he wanted me to build --- a chatbot and he went away.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "nopunct = ''\n",
    "for char in sample_str:\n",
    "    if(char not in punct):\n",
    "        nopunct = nopunct + char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi  he wanted me to build  a chatbot and he went away'"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nopunct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regex to clean to avoind for loop and work faster\n",
    "\n",
    "import re \n",
    "sampstr= \"should. I. Build?\"\n",
    "sampstr= re.sub(r'[^\\w\\s]','',sampstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'should I Build'"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampstr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LETS Tokenize\n",
    "#### -breaking down of sentences into word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\krant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\krant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download ('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lets', 'have', 'some', 'fun', 'baby', '!']"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize('lets have some fun baby!') #its basically uses python split function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STOP WORD REMOVING \n",
    "##### - removes the word which doesnt hold much value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.append('wwwwwwkkkkk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'wwwwwwkkkkk']"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "#### - To reduce the words to the word stems like males to male , playing to play ---if we dont do this male and males will process as different words\n",
    "\n",
    "#### may not be efficient method but simple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bot\n",
      "are\n",
      "the\n",
      "futur\n",
      "assist\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "mystr = ' Bots are the future assistants.'\n",
    "mystr = nltk.word_tokenize(mystr)\n",
    "for word in mystr:\n",
    "    print (stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization - \n",
    "####  efficient way of reducing a word to its root form , often time its harder to create lemmatizer with new language then stemming  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bots\n",
      "are\n",
      "the\n",
      "future\n",
      "assistant\n",
      ",\n",
      "but\n",
      "mouse\n",
      "were\n",
      "everywhere\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "mystr = ' Bots are the future assistants, but mice were everywhere'\n",
    "mystr = nltk.word_tokenize(mystr)\n",
    "for word in mystr:\n",
    "    print(lem.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets apply these data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    Create new account and share the username and ...\n",
       "2                       FW: Myspace Login Issue-- Ania\n",
       "3                    Login credetials are not working.\n",
       "4              New Account is needed for a new joinee.\n",
       "Name: Title, dtype: object"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Title'][1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets apply to our data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'\\nlem = WordNetLemmatizer()\\nfor index,row in df.iterrows():\\n    filter_text = []\\n    text = row['Title']\\n    text = re.sub(r'[^\\\\w\\\\s]','',text) #cleaning with reg EX\\n    words = nltk.word_tokenize(text) # tokenization\\n    words = [w for w in words if not w in stop_words] #stopword removal\\n    \\n    #print(words)\\n    #lemmatization\\n    for word in words :\\n        filter_text.append(lem.lemmatize(word))\\n        #filter_text = filter_text + ' '+ lem.lemmatize(word)\\n    print(filter_text)\\n    #df.ix[index , 'Title'] = filter_text\\n \\n#if any suspicious words are present like other languages , symbols ets , you can add those to the stopwords and remove that \\n# lets delemmatize this \\n\""
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''''\n",
    "lem = WordNetLemmatizer()\n",
    "for index,row in df.iterrows():\n",
    "    filter_text = []\n",
    "    text = row['Title']\n",
    "    text = re.sub(r'[^\\w\\s]','',text) #cleaning with reg EX\n",
    "    words = nltk.word_tokenize(text) # tokenization\n",
    "    words = [w for w in words if not w in stop_words] #stopword removal\n",
    "    \n",
    "    #print(words)\n",
    "    #lemmatization\n",
    "    for word in words :\n",
    "        filter_text.append(lem.lemmatize(word))\n",
    "        #filter_text = filter_text + ' '+ lem.lemmatize(word)\n",
    "    print(filter_text)\n",
    "    #df.ix[index , 'Title'] = filter_text\n",
    " \n",
    "#if any suspicious words are present like other languages , symbols ets , you can add those to the stopwords and remove that \n",
    "# lets delemmatize this \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets analyse by applying NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some of the techniques we need to know before going is BOW , TF-idf , W2V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"its main aim is to remove the poverty and \\ enable the dignified life.\"\n",
    "\n",
    "count_vec = CountVectorizer() # distinct words\n",
    "count_occurs = count_vec.fit_transform([text]) # it holds the frequency of each word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>the</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aim</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dignified</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enable</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word  Count\n",
       "10        the      2\n",
       "0         aim      1\n",
       "1         and      1\n",
       "2   dignified      1\n",
       "3      enable      1"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets create a data frame \n",
    "\n",
    "count_occur_df = pd.DataFrame(\n",
    "    \n",
    "    (count,word) for word,count in  \n",
    "      zip(count_occurs.toarray().tolist()[0],\n",
    "     count_vec.get_feature_names()))\n",
    "\n",
    "count_occur_df.columns = ['Word' , 'Count']\n",
    "count_occur_df.sort_values('Count', ascending = False , inplace = True)\n",
    "count_occur_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apply', 'code', 'country', 'docoument', 'document', 'does', 'india', 'is', 'makes', 'my', 'please', 'sence', 'that', 'the', 'we']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [ \n",
    "    \n",
    "    'the document is the document ', 'india is my country ', 'please docoument the code that we apply.', ' does makes sence?'\n",
    "]\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 2 0 0 1 0 0 0 0 0 2 0]\n",
      " [0 0 1 0 0 0 1 1 0 1 0 0 0 0 0]\n",
      " [1 1 0 1 0 0 0 0 0 0 1 0 1 1 1]\n",
      " [0 0 0 0 0 1 0 0 1 0 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray()) # each row indicates a sentence and eaach value indiacate frequency of word  whioch is in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['code that', 'docoument the', 'document is', 'does makes', 'india is', 'is my', 'is the', 'makes sence', 'my country', 'please docoument', 'that we', 'the code', 'the document', 'we apply']\n",
      "[[0 0 1 0 0 0 1 0 0 0 0 0 2 0]\n",
      " [0 0 0 0 1 1 0 0 1 0 0 0 0 0]\n",
      " [1 1 0 0 0 0 0 0 0 1 1 1 0 1]\n",
      " [0 0 0 1 0 0 0 1 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#another example\n",
    "#ngram will take two words as two words #it only works with the unique word\n",
    "vectorizer2 = CountVectorizer(analyzer = 'word' , ngram_range =(2,2))\n",
    "X2 = vectorizer2.fit_transform(corpus)\n",
    "print(vectorizer2.get_feature_names())\n",
    "print(X2.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-IDF\n",
    "\n",
    "### - Term freq(tf) = count(each word)/ total words\n",
    "\n",
    "### - inverse doc freq(IDF) = log(N/df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eX -\n",
    "\n",
    "a = 'the climate is good'\n",
    "b = 'the climate is hot'\n",
    "\n",
    "bow_a = a.split(' ')\n",
    "bow_b = b.split (' ')\n",
    "#tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'climate', 'is', 'good']"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "unqwords = set(bow_a).union(set(bow_b))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'climate', 'good', 'hot', 'is', 'the'}"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unqwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    " #creating a dictionary with count of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_words_a = dict.fromkeys(unqwords,0)\n",
    "for word in bow_a :\n",
    "    no_words_a[word] +=1\n",
    "no_words_b = dict.fromkeys(unqwords, 0 )\n",
    "for word in bow_b:\n",
    "    no_words_b[word] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 1, 'hot': 0, 'the': 1, 'climate': 1, 'good': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'is': 1, 'hot': 1, 'the': 1, 'climate': 1, 'good': 0}"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(no_words_a)\n",
    "no_words_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict , bow):\n",
    "    tfDict = {}\n",
    "    bowcount = len(bow)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bowcount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfa = computeTF(no_words_a,bow_a)\n",
    "tfb = computeTF(no_words_b, bow_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is': 0.25, 'hot': 0.0, 'the': 0.25, 'climate': 0.25, 'good': 0.25}"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is': 0.25, 'hot': 0.25, 'the': 0.25, 'climate': 0.25, 'good': 0.0}"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarly for idf ]\n",
    "#then multiply it with TF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR WE have sklearn lib which is pre implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(df['Title']) #fit will create the dictionory ad transform will applky tfidf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<236x270 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1596 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['able',\n",
       " 'access',\n",
       " 'account',\n",
       " 'act',\n",
       " 'acting',\n",
       " 'addresses',\n",
       " 'allowed',\n",
       " 'am',\n",
       " 'among',\n",
       " 'an',\n",
       " 'and',\n",
       " 'ania',\n",
       " 'anti',\n",
       " 'any',\n",
       " 'anymore',\n",
       " 'app',\n",
       " 'appearing',\n",
       " 'appears',\n",
       " 'application',\n",
       " 'are',\n",
       " 'at',\n",
       " 'attachment',\n",
       " 'attachments',\n",
       " 'attempts',\n",
       " 'authorization',\n",
       " 'available',\n",
       " 'battery',\n",
       " 'be',\n",
       " 'because',\n",
       " 'beginning',\n",
       " 'behave',\n",
       " 'behaves',\n",
       " 'behaving',\n",
       " 'being',\n",
       " 'between',\n",
       " 'bin',\n",
       " 'black',\n",
       " 'blocked',\n",
       " 'bloqueó',\n",
       " 'blue',\n",
       " 'borders',\n",
       " 'by',\n",
       " 'can',\n",
       " 'cause',\n",
       " 'clean',\n",
       " 'client',\n",
       " 'closing',\n",
       " 'come',\n",
       " 'compte',\n",
       " 'computer',\n",
       " 'configuration',\n",
       " 'configure',\n",
       " 'configured',\n",
       " 'connect',\n",
       " 'connection',\n",
       " 'contraseña',\n",
       " 'cookies',\n",
       " 'correctly',\n",
       " 'could',\n",
       " 'crashes',\n",
       " 'create',\n",
       " 'credentials',\n",
       " 'credetials',\n",
       " 'cuenta',\n",
       " 'damage',\n",
       " 'database',\n",
       " 'de',\n",
       " 'debido',\n",
       " 'delete',\n",
       " 'desktop',\n",
       " 'determine',\n",
       " 'disc',\n",
       " 'disk',\n",
       " 'display',\n",
       " 'distorted',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " 'doing',\n",
       " 'down',\n",
       " 'drive',\n",
       " 'due',\n",
       " 'eliminate',\n",
       " 'email',\n",
       " 'emails',\n",
       " 'enter',\n",
       " 'equipped',\n",
       " 'erase',\n",
       " 'erratically',\n",
       " 'ethernet',\n",
       " 'expiration',\n",
       " 'expired',\n",
       " 'expiry',\n",
       " 'faced',\n",
       " 'facing',\n",
       " 'fails',\n",
       " 'favor',\n",
       " 'fi',\n",
       " 'file',\n",
       " 'find',\n",
       " 'fix',\n",
       " 'for',\n",
       " 'forgot',\n",
       " 'forgotten',\n",
       " 'format',\n",
       " 'freezes',\n",
       " 'from',\n",
       " 'fw',\n",
       " 'get',\n",
       " 'getting',\n",
       " 'given',\n",
       " 'got',\n",
       " 'hangs',\n",
       " 'hard',\n",
       " 'hardware',\n",
       " 'how',\n",
       " 'if',\n",
       " 'image',\n",
       " 'impossible',\n",
       " 'in',\n",
       " 'inactive',\n",
       " 'incorrectos',\n",
       " 'input',\n",
       " 'install',\n",
       " 'installation',\n",
       " 'installer',\n",
       " 'intentos',\n",
       " 'internet',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " 'issue',\n",
       " 'it',\n",
       " 'jam',\n",
       " 'joinee',\n",
       " 'keyboard',\n",
       " 'la',\n",
       " 'lan',\n",
       " 'laptop',\n",
       " 'le',\n",
       " 'locked',\n",
       " 'log',\n",
       " 'login',\n",
       " 'lost',\n",
       " 'macbook',\n",
       " 'machine',\n",
       " 'mail',\n",
       " 'map',\n",
       " 'mauvaises',\n",
       " 'me',\n",
       " 'member',\n",
       " 'message',\n",
       " 'microsoft',\n",
       " 'middle',\n",
       " 'mode',\n",
       " 'monitor',\n",
       " 'mot',\n",
       " 'mouse',\n",
       " 'ms',\n",
       " 'my',\n",
       " 'myspace',\n",
       " 'name',\n",
       " 'necessary',\n",
       " 'need',\n",
       " 'needed',\n",
       " 'needs',\n",
       " 'network',\n",
       " 'new',\n",
       " 'no',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'of',\n",
       " 'off',\n",
       " 'office',\n",
       " 'on',\n",
       " 'or',\n",
       " 'os',\n",
       " 'outlook',\n",
       " 'pages',\n",
       " 'paper',\n",
       " 'participant',\n",
       " 'passe',\n",
       " 'password',\n",
       " 'permission',\n",
       " 'please',\n",
       " 'podrías',\n",
       " 'por',\n",
       " 'possible',\n",
       " 'postgres',\n",
       " 'pourriez',\n",
       " 'power',\n",
       " 'printer',\n",
       " 'printing',\n",
       " 'prints',\n",
       " 'problem',\n",
       " 'program',\n",
       " 'properly',\n",
       " 'protection',\n",
       " 'qa',\n",
       " 'receive',\n",
       " 'recover',\n",
       " 'recycle',\n",
       " 'reiniciar',\n",
       " 'remove',\n",
       " 'replace',\n",
       " 'request',\n",
       " 'required',\n",
       " 'requires',\n",
       " 'reset',\n",
       " 'réinitialiser',\n",
       " 'safe',\n",
       " 'screen',\n",
       " 'se',\n",
       " 'send',\n",
       " 'share',\n",
       " 'should',\n",
       " 'showing',\n",
       " 'shows',\n",
       " 'shut',\n",
       " 'shutting',\n",
       " 'signal',\n",
       " 'skewed',\n",
       " 'smeared',\n",
       " 'smudged',\n",
       " 'space',\n",
       " 'spaces',\n",
       " 'stained',\n",
       " 'start',\n",
       " 'startup',\n",
       " 'strangely',\n",
       " 'switched',\n",
       " 'system',\n",
       " 'tentatives',\n",
       " 'that',\n",
       " 'the',\n",
       " 'there',\n",
       " 'tilted',\n",
       " 'to',\n",
       " 'turn',\n",
       " 'unable',\n",
       " 'unpredictably',\n",
       " 'up',\n",
       " 'update',\n",
       " 'used',\n",
       " 'username',\n",
       " 'users',\n",
       " 'using',\n",
       " 'verrouillé',\n",
       " 'virtual',\n",
       " 'virus',\n",
       " 'vm',\n",
       " 'vous',\n",
       " 'was',\n",
       " 'what',\n",
       " 'wheel',\n",
       " 'when',\n",
       " 'why',\n",
       " 'wi',\n",
       " 'wifi',\n",
       " 'will',\n",
       " 'windows',\n",
       " 'with',\n",
       " 'without',\n",
       " 'work',\n",
       " 'working',\n",
       " 'wrong',\n",
       " 'xwq090',\n",
       " 'you',\n",
       " 'été']"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets build text classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features , labels \n",
    "#train and validation\n",
    "\n",
    "#label encoding , train text splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection , preprocessing #\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train , x_valid , y_train , y_valid = model_selection.train_test_split(vectors , df['Resolution'], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188, 270) (48, 270)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape , x_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9      We regret for the inconvinience caused. The is...\n",
       "102    Press F8 button on system start, Choose safe m...\n",
       "199     Thankfully a missing printer installation dis...\n",
       "168     As with most computer errors, your first step...\n",
       "234     Users who have an optical mechanical mouse (m...\n",
       "145    changed network settings to Bridged and config...\n",
       "99      Make sure both the computer and monitor are o...\n",
       "76      Microsoft Internet Explorer users can go to '...\n",
       "135     Verify that the network cable is properly con...\n",
       "51      If you're running any version of Microsoft Wi...\n",
       "186     The keys on a keyboard are only clipped on, w...\n",
       "166     If the resolution was recently changed it is ...\n",
       "75      If you are running Microsoft Windows 7, Windo...\n",
       "177    Go to URL : accountsetup.com, Enter personal d...\n",
       "139    Go to URL : password.reset.com, Enter you logi...\n",
       "215    Go to network tab from bottom right corner, ch...\n",
       "69     Go to network tab from bottom right corner, ch...\n",
       "148    Go to network tab from bottom right corner, ch...\n",
       "183    Go to URL : password.reset.com, Enter you logi...\n",
       "61     Unlocked the account and new password is sent ...\n",
       "71      The keys on a keyboard are only clipped on, w...\n",
       "8      First, connect a network cable to your laptop ...\n",
       "19     Go to network tab from bottom right corner, ch...\n",
       "136     Close all programs running on the computer an...\n",
       "129     If you are running Microsoft Windows 7, Windo...\n",
       "86     Go to \\imfs\\software\\postgres location, copy p...\n",
       "60      Just like an Internet URL no spaces are allow...\n",
       "104    1. Hold the power button until system gets shu...\n",
       "100    Go to URL : password.reset.com, Enter you logi...\n",
       "219    They won't be able to access other profiles if...\n",
       "42      Verify that the monitor is properly connected...\n",
       "83      Thankfully a missing printer installation dis...\n",
       "142     If your computer is infected with a virus for...\n",
       "103    They won't be able to access other profiles if...\n",
       "15     Go to network tab from bottom right corner, ch...\n",
       "41      If the e-mail box is full of other e-mail mes...\n",
       "105    We regret for the inconvinience caused. The is...\n",
       "108    Press F8 button on system start, Choose safe m...\n",
       "45      Users should not simply press the power butto...\n",
       "125    Go to URL : password.reset.com, Enter you logi...\n",
       "67      All versions of Microsoft Windows do not come...\n",
       "165     If you're running any version of Microsoft Wi...\n",
       "109     If the resolution was recently changed it is ...\n",
       "33      Microsoft Internet Explorer users can go to '...\n",
       "133     To get into the Windows 7 / 10 Safe mode, as ...\n",
       "137     If the e-mail box is full of other e-mail mes...\n",
       "220    First, connect a network cable to your laptop ...\n",
       "74      Double click the recycle bin icon on your des...\n",
       "Name: Resolution, dtype: object"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can't give labels as a resulution to ml algo so encoding \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding to the target variable \n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_valid = encoder.transform(y_valid) # we are not uysing fit(which created dict in valid becz we want follow the same dict )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([48, 45, 19,  2, 26, 49, 13, 15, 28, 10, 20,  8,  9, 35, 36, 38, 38,\n",
       "       38, 36, 47, 20, 34, 38,  4,  9, 37, 12, 32, 36, 46, 27, 19, 11, 46,\n",
       "       38,  7, 48, 45, 25, 36,  1, 10,  8, 15, 22,  7, 34,  5])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets start modelling - Logistic reg\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(x_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = lgr.predict(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of log reg classifier on test set : 0.75\n"
     ]
    }
   ],
   "source": [
    "print('accuracy of log reg classifier on test set : {:.2f}'.format(metrics.accuracy_score(y_predict , y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([38])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgr.predict(vectorizer.transform(['my keybourd is not working'])) # if i fit again it created another dictionor and labels may change may be due to new words which our model not seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do much advanced processing and modelling for better accuracy then we can make flask app with this pipe lik"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
